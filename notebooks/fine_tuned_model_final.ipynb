{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c79990",
   "metadata": {},
   "source": [
    "## I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71333b5c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3eeb004",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T02:04:30.432577Z",
     "iopub.status.busy": "2025-08-04T02:04:30.432237Z",
     "iopub.status.idle": "2025-08-04T02:04:30.438445Z",
     "shell.execute_reply": "2025-08-04T02:04:30.437647Z",
     "shell.execute_reply.started": "2025-08-04T02:04:30.432548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification, TrainingArguments,Trainer\n",
    "from datasets import load_dataset,DatasetDict\n",
    "import torch\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4614ad50",
   "metadata": {},
   "source": [
    "# Loading model, tokenizer and splitting the dataset\n",
    "\n",
    "The task at hand that I have first is to load the model, its tokenizer and then the dataset, but also I have to split the dataset, changes it labels, becuase labels are in string format rather than in number, so have to change it using ClassLabel and then also to change the name of the columns to follow the format that the `Trainer` class expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb3e631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T02:04:30.611225Z",
     "iopub.status.busy": "2025-08-04T02:04:30.611019Z",
     "iopub.status.idle": "2025-08-04T02:04:58.002575Z",
     "shell.execute_reply": "2025-08-04T02:04:58.001865Z",
     "shell.execute_reply.started": "2025-08-04T02:04:30.611207Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd526e0549674cbf9ecb32e9f2468b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating full split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8bba5de6b24c8c82f857337185b7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/487235 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "\n",
    "# constants I will use later on\n",
    "data_path = \"/Users/kannavsethi/Desktop/nlp-final-project/data/AI_Human.csv\"\n",
    "text_column = \"text\"\n",
    "label_column = \"generated\"\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "raw_data = load_dataset(\"csv\", data_files={\"full\": data_path})\n",
    "\n",
    "# renaming the columns\n",
    "raw_data = raw_data.rename_column(label_column, \"label\")\n",
    "\n",
    "# what this does is to convert the label column into a ClassLabel type\n",
    "# which is a special type used by the datasets library to handle labels\n",
    "labels = raw_data[\"full\"].unique(\"label\")\n",
    "raw_data = raw_data.cast_column(\"label\", ClassLabel(names=labels))\n",
    "\n",
    "# loading the full dataset and then using the train_test_split method to split it into two halfs, testing and training\n",
    "d0 = raw_data[\"full\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# same thing, but now testing and validation\n",
    "d1 = d0[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# creating a DatasetDict to hold the train, validation, and test sets, much easier for me to look into it\n",
    "dataset = DatasetDict({\n",
    "    \"train\": d0[\"train\"],       # 80%\n",
    "    \"validation\": d1[\"train\"],  # 10%\n",
    "    \"test\": d1[\"test\"],         # 10%\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ee13e",
   "metadata": {},
   "source": [
    "# Creating the custom tokenizer function that we will be using for batched inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10459380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T02:04:58.003481Z",
     "iopub.status.busy": "2025-08-04T02:04:58.003270Z",
     "iopub.status.idle": "2025-08-04T02:11:41.362126Z",
     "shell.execute_reply": "2025-08-04T02:11:41.361275Z",
     "shell.execute_reply.started": "2025-08-04T02:04:58.003463Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6f31fdbe514e729bb2b4244b787ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838dd8150377423b90f271983f9b6c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66567f1ca8e48c5881ccea48b21abc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa075e0b0404f2abe76345e30d63f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5146e650d2294f56a0d29ea645a9b3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/389788 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a8490dcc224a4594c221231a44237c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48723 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117241f761234c4ebc8f577be7651e64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/48724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e747ed5",
   "metadata": {},
   "source": [
    "# Trainer and Training Arguments\n",
    "\n",
    "To fine-tune, I will be using the Trainer class from transformers and corresponding training_arguments class as well to specify parameters for the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef7787b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T02:11:41.371160Z",
     "iopub.status.busy": "2025-08-04T02:11:41.370891Z",
     "iopub.status.idle": "2025-08-04T12:04:30.506525Z",
     "shell.execute_reply": "2025-08-04T12:04:30.505932Z",
     "shell.execute_reply.started": "2025-08-04T02:11:41.371144Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b4b0da46484d91a319d3f33589fb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_36/2783823326.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36543' max='36543' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36543/36543 9:52:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>0.999405</td>\n",
       "      <td>0.999201</td>\n",
       "      <td>0.999339</td>\n",
       "      <td>0.999063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001430</td>\n",
       "      <td>0.999651</td>\n",
       "      <td>0.999532</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>0.999394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.999815</td>\n",
       "      <td>0.999752</td>\n",
       "      <td>0.999780</td>\n",
       "      <td>0.999725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36543, training_loss=0.00578049455844403, metrics={'train_runtime': 35565.9295, 'train_samples_per_second': 32.879, 'train_steps_per_second': 1.027, 'total_flos': 1.549026071619748e+17, 'train_loss': 0.00578049455844403, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metrics that i made in a separate file as they are used in multiple places, so wanted them to be modular\n",
    "from evaluation_metrics import compute_metrics_for_trainer, evaluate_model\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 2)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./models\", # place to store the model checkpoints\n",
    "    eval_strategy=\"epoch\", # evaluate the model at the end of each epoch\n",
    "    learning_rate=2e-5, # learning rate for the optimizer\n",
    "    per_device_train_batch_size=16, # batch size for training\n",
    "    per_device_eval_batch_size=16, # batch size for evaluation\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    weight_decay=0.01, # weight decay for regularization\n",
    "    load_best_model_at_end=True, # load the best model at the end of training\n",
    "    logging_dir=\"./logs\", # directory for storing logs\n",
    "    logging_steps=10, # log every 10 steps\n",
    "    save_strategy=\"epoch\", # save the model at the end of each epoch\n",
    "    report_to=\"none\" # i don't want the wandb thing to show up, so I set it to none\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, # the model to train\n",
    "    args=training_args, # the training arguments\n",
    "    train_dataset=tokenized_datasets[\"train\"], # the training dataset\n",
    "    eval_dataset=tokenized_datasets[\"validation\"], # the validation dataset\n",
    "    tokenizer=tokenizer, # the tokenizer to use\n",
    "    compute_metrics=compute_metrics_for_trainer # the function to compute metrics during evaluation, again this is the custom function that I made\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439311c3",
   "metadata": {},
   "source": [
    "# Finally evaluating and seeing detailed metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b6b534-3e3e-40a5-bed5-b71aa7859c09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-04T12:04:30.507546Z",
     "iopub.status.busy": "2025-08-04T12:04:30.507304Z",
     "iopub.status.idle": "2025-08-04T12:20:09.604377Z",
     "shell.execute_reply": "2025-08-04T12:20:09.603773Z",
     "shell.execute_reply.started": "2025-08-04T12:04:30.507528Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 0.0013374830596148968, 'eval_accuracy': 0.9997331910352187, 'eval_f1': 0.9996445076430859, 'eval_precision': 0.9996171725458026, 'eval_recall': 0.9996718442353971, 'eval_runtime': 469.88, 'eval_samples_per_second': 103.695, 'eval_steps_per_second': 3.241, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Test Metrics:\n",
      "Accuracy: 0.9997331910352187\n",
      "F1 Score: 0.9996445076430859\n",
      "Precision: 0.9996171725458026\n",
      "Recall: 0.9996718442353971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9997331910352187,\n",
       " 'f1_score': 0.9996445076430859,\n",
       " 'precision': 0.9996171725458026,\n",
       " 'recall': 0.9996718442353971}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# evaluating the model on the test set\n",
    "results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"Test Results:\", results)\n",
    "\n",
    "# making predictions on the test set\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "y_pred = predictions.predictions.argmax(-1)\n",
    "y_true = predictions.label_ids\n",
    "\n",
    "# printing the classification report\n",
    "print(\"\\nDetailed Test Metrics:\")\n",
    "evaluate_model(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4288635,
     "sourceId": 7379779,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
